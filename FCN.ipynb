{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSM5Xhv1CT/HEivAEK1cPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chimaOkwuokei/ISEND-PAU/blob/main/FCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Uz5-1jekQeO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read ImageNet class id to name mapping\n",
        "    with open('imagenet_classes.txt') as f:\n",
        "        labels = [line.strip() for line in f.readlines()]"
      ],
      "metadata": {
        "id": "dxcrXHQ3kb6Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Read image\n",
        "original_image = cv2.imread('camel.jpg')\n",
        "\n",
        "# Convert original image to RGB format\n",
        "image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Transform input image\n",
        "# 1. Convert to Tensor\n",
        "# 2. Subtract mean\n",
        "# 3. Divide by standard deviation\n",
        "\n",
        "transform = transforms.Compose([\n",
        "             transforms.ToTensor(), #Convert image to tensor.\n",
        "             transforms.Normalize(\n",
        "             mean=[0.485, 0.456, 0.406],   # Subtract mean\n",
        "             std=[0.229, 0.224, 0.225]     # Divide by standard deviation\n",
        "             )])\n",
        "\n",
        "image = transform(image)\n",
        "image = image.unsqueeze(0)\n",
        "print(transform)\n",
        "image"
      ],
      "metadata": {
        "id": "R0TTd7g2kwvu",
        "outputId": "084c1bd9-17d6-4637-b6b2-48be23e3ed94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.1426,  0.1597,  0.1597,  ...,  0.5536,  0.5364,  0.5364],\n",
              "          [ 0.1597,  0.1768,  0.1768,  ...,  0.5707,  0.5707,  0.5707],\n",
              "          [ 0.1597,  0.1768,  0.1939,  ...,  0.5878,  0.5878,  0.5878],\n",
              "          ...,\n",
              "          [-1.9467,  0.6049,  2.0434,  ...,  0.4166,  1.1358,  0.8618],\n",
              "          [-1.9467,  0.5878,  2.0434,  ...,  0.6906, -0.0458,  0.1083],\n",
              "          [-1.9980,  0.4679,  1.9407,  ..., -0.3369,  0.5536, -0.1314]],\n",
              "\n",
              "         [[ 1.0455,  1.0630,  1.0630,  ...,  1.2381,  1.2206,  1.2206],\n",
              "          [ 1.0630,  1.0805,  1.0805,  ...,  1.2556,  1.2556,  1.2556],\n",
              "          [ 1.0630,  1.0805,  1.0980,  ...,  1.2731,  1.2731,  1.2731],\n",
              "          ...,\n",
              "          [-1.9832,  0.6078,  2.0084,  ..., -0.0399,  0.6779,  0.3978],\n",
              "          [-1.9832,  0.5903,  2.0084,  ...,  0.2402, -0.5301, -0.3725],\n",
              "          [-2.0357,  0.4678,  1.9034,  ..., -0.8102,  0.0826, -0.6176]],\n",
              "\n",
              "         [[ 1.8731,  1.8905,  1.8905,  ...,  1.7685,  1.7511,  1.7511],\n",
              "          [ 1.8905,  1.9080,  1.9080,  ...,  1.7860,  1.7860,  1.7860],\n",
              "          [ 1.8905,  1.9080,  1.9254,  ...,  1.8034,  1.8034,  1.8034],\n",
              "          ...,\n",
              "          [-1.8044,  0.4614,  1.8034,  ..., -0.2881,  0.4091,  0.0953],\n",
              "          [-1.8044,  0.4439,  1.8034,  ..., -0.0092, -0.7936, -0.6715],\n",
              "          [-1.8044,  0.3219,  1.6988,  ..., -1.0550, -0.1835, -0.9156]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# Define the architecture by modifying resnet.\n",
        "# Original code is here\n",
        "# https://github.com/pytorch/vision/blob/b2e95657cd5f389e3973212ba7ddbdcc751a7878/torchvision/models/resnet.py\n",
        "class FullyConvolutionalResnet18(models.ResNet):\n",
        "    def __init__(self, num_classes=1000, pretrained=False, **kwargs):\n",
        "\n",
        "        # Start with standard resnet18 defined here\n",
        "        # https://github.com/pytorch/vision/blob/b2e95657cd5f389e3973212ba7ddbdcc751a7878/torchvision/models/resnet.py\n",
        "        super().__init__(block=models.resnet.BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes, **kwargs)\n",
        "        if pretrained:\n",
        "            state_dict = load_state_dict_from_url(models.resnet.model_urls[\"resnet18\"], progress=True)\n",
        "            self.load_state_dict(state_dict)\n",
        "\n",
        "        # Replace AdaptiveAvgPool2d with standard AvgPool2d\n",
        "        # https://github.com/pytorch/vision/blob/b2e95657cd5f389e3973212ba7ddbdcc751a7878/torchvision/models/resnet.py#L153-L154\n",
        "        self.avgpool = nn.AvgPool2d((7, 7))\n",
        "\n",
        "        # Add final Convolution Layer.\n",
        "        self.last_conv = torch.nn.Conv2d(in_channels=self.fc.in_features, out_channels=num_classes, kernel_size=1)\n",
        "        self.last_conv.weight.data.copy_(self.fc.weight.data.view(*self.fc.weight.data.shape, 1, 1))\n",
        "        self.last_conv.bias.data.copy_(self.fc.bias.data)\n",
        "\n",
        "    # Reimplementing forward pass.\n",
        "    # Replacing the following code\n",
        "    # https://github.com/pytorch/vision/blob/b2e95657cd5f389e3973212ba7ddbdcc751a7878/torchvision/models/resnet.py#L197-L213\n",
        "    def _forward_impl(self, x):\n",
        "        # Standard forward for resnet18\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        # Notice, there is no forward pass\n",
        "        # through the original fully connected layer.\n",
        "        # Instead, we forward pass through the last conv layer\n",
        "        x = self.last_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read ImageNet class id to name mapping\n",
        "    with open('imagenet_classes.txt') as f:\n",
        "        labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # Read image\n",
        "    original_image = cv2.imread('camel.jpg')\n",
        "\n",
        "    # Convert original image to RGB format\n",
        "    image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Transform input image\n",
        "    # 1. Convert to Tensor\n",
        "    # 2. Subtract mean\n",
        "    # 3. Divide by standard deviation\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "                 transforms.ToTensor(), #Convert image to tensor.\n",
        "                 transforms.Normalize(\n",
        "                 mean=[0.485, 0.456, 0.406],   # Subtract mean\n",
        "                 std=[0.229, 0.224, 0.225]     # Divide by standard deviation\n",
        "                 )])\n",
        "\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    # Load modified resnet18 model with pretrained ImageNet weights\n",
        "    model = FullyConvolutionalResnet18(pretrained=True).eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Perform inference.\n",
        "        # Instead of a 1x1000 vector, we will get a\n",
        "        # 1x1000xnxm output ( i.e. a probabibility map\n",
        "        # of size n x m for each 1000 class,\n",
        "        # where n and m depend on the size of the image.)\n",
        "        preds = model(image)\n",
        "        preds = torch.softmax(preds, dim=1)\n",
        "\n",
        "        print('Response map shape : ', preds.shape)\n",
        "\n",
        "        # Find the class with the maximum score in the n x m output map\n",
        "        pred, class_idx = torch.max(preds, dim=1)\n",
        "        print(class_idx)\n",
        "\n",
        "\n",
        "        row_max, row_idx = torch.max(pred, dim=1)\n",
        "        col_max, col_idx = torch.max(row_max, dim=1)\n",
        "        predicted_class = class_idx[0, row_idx[0, col_idx], col_idx]\n",
        "\n",
        "        # Print top predicted class\n",
        "        print('Predicted Class : ', labels[predicted_class], predicted_class)\n",
        "\n",
        "        # Find the n x m score map for the predicted class\n",
        "        score_map = preds[0, predicted_class, :, :].cpu().numpy()\n",
        "        score_map = score_map[0]\n",
        "\n",
        "        # Resize score map to the original image size\n",
        "        score_map = cv2.resize(score_map, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "        # Binarize score map\n",
        "        _, score_map_for_contours = cv2.threshold(score_map, 0.25, 1, type=cv2.THRESH_BINARY)\n",
        "        score_map_for_contours = score_map_for_contours.astype(np.uint8).copy()\n",
        "\n",
        "        # Find the countour of the binary blob\n",
        "        contours, _ = cv2.findContours(score_map_for_contours, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Find bounding box around the object.\n",
        "        rect = cv2.boundingRect(contours[0])\n",
        "\n",
        "        # Apply score map as a mask to original image\n",
        "        score_map = score_map - np.min(score_map[:])\n",
        "        score_map = score_map / np.max(score_map[:])\n",
        "\n",
        "        score_map = cv2.cvtColor(score_map, cv2.COLOR_GRAY2BGR)\n",
        "        masked_image = (original_image * score_map).astype(np.uint8)\n",
        "\n",
        "        # Display bounding box\n",
        "        cv2.rectangle(masked_image, rect[:2], (rect[0] + rect[2], rect[1] + rect[3]), (0, 0, 255), 2)\n",
        "\n",
        "        # Display images\n",
        "        cv2.imshow(\"Original Image\", original_image)\n",
        "        cv2.imshow(\"scaled_score_map\", score_map)\n",
        "        cv2.imshow(\"activations_and_bbox\", masked_image)\n",
        "        cv2.waitKey(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVAkm8Nrmwju",
        "outputId": "2dde6484-9daf-44cf-e37e-4ff66394db5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torchvision.models.resnet' has no attribute 'model_urls'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1a5a73ef067f>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Load modified resnet18 model with pretrained ImageNet weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConvolutionalResnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-1a5a73ef067f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resnet18\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models.resnet' has no attribute 'model_urls'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load modified resnet18 model with pretrained ImageNet weights\n",
        "# model = FullyConvolutionalResnet18(pretrained=True).eval()"
      ],
      "metadata": {
        "id": "YuilgvP9lv_U",
        "outputId": "572bf187-498e-4506-8eff-9a4bb06f15fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FullyConvolutionalResnet18' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2837f0ca7ddb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load modified resnet18 model with pretrained ImageNet weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConvolutionalResnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'FullyConvolutionalResnet18' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# Function to load the pre-trained FCN ResNet50 model\n",
        "def load_fcn_resnet50(pretrained=True, num_classes=1000):\n",
        "    model = models.segmentation.fcn_resnet50(pretrained=pretrained)\n",
        "    # If you want to adjust for a different number of classes, modify the final classifier\n",
        "    if num_classes != 1000:\n",
        "        model.classifier[4] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Read ImageNet class id to name mapping\n",
        "    with open('imagenet_classes.txt') as f:\n",
        "        labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # Read image\n",
        "    original_image = cv2.imread('camel.jpg')\n",
        "\n",
        "    # Convert original image to RGB format\n",
        "    image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Transform input image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)\n",
        "\n",
        "    # Load FCN-ResNet50 model with pretrained weights\n",
        "    model = load_fcn_resnet50(pretrained=True).eval()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ViDILkEmkP7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # Perform inference\n",
        "    output = model(image)\n",
        "\n",
        "    # Extract the prediction map from the output (it is stored in the 'out' key)\n",
        "    preds = output['out']\n",
        "\n",
        "    # Apply softmax to the output tensor along the class dimension\n",
        "    preds = torch.softmax(preds, dim=1)\n",
        "\n",
        "    print('Response map shape : ', preds.shape)\n",
        "\n",
        "    # Get the predicted class per pixel\n",
        "    pred = torch.argmax(preds, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Use the predicted class to create a mask\n",
        "    predicted_class = np.unique(pred)[0]  # Assume the dominant class in the image\n",
        "\n",
        "    print(f'Predicted Class: {labels[predicted_class]}')\n",
        "\n",
        "    # Find the score map for the predicted class\n",
        "    score_map = preds[0, predicted_class, :, :].cpu().numpy()\n"
      ],
      "metadata": {
        "id": "NKKhxG_RnfR9",
        "outputId": "e1935b55-b6a9-415e-bc65-502a52163e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response map shape :  torch.Size([1, 21, 725, 1920])\n",
            "Predicted Class: tench, Tinca tinca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VC48bV5xoEb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}